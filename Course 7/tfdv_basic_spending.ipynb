{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Data Validation\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Review TFDV methods\n",
    "2. Generate statistics\n",
    "3. Visualize statistics\n",
    "4. Infer a schema\n",
    "5. Update a schema\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "This lab is an introduction to TensorFlow Data Validation (TFDV), a key component of TensorFlow Extended.  This lab serves as a foundation for understanding the features of TFDV and how it can help you understand, validate, and monitor your data. \n",
    "\n",
    "TFDV can be used for generating schemas and statistics about the distribution of every feature in the dataset. Such information is useful for comparing multiple datasets (e.g. training vs inference datasets) and reporting:\n",
    "\n",
    "Statistical differences in the features distribution\n",
    "TFDV also offers visualization capabilities for comparing datasets based on the Google PAIR Facets project.  \n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [Solution Notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/production_ml/solutions/tfdv_basic_spending.ipynb) for reference. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow==5.0.0\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow==5.0.0) (1.19.5)\n",
      "Installing collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 8.0.0\n",
      "    Uninstalling pyarrow-8.0.0:\n",
      "      Successfully uninstalled pyarrow-8.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pyarrow-5.0.0\n",
      "Collecting numpy==1.19.2\n",
      "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\n",
      "tfx-bsl 1.9.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.52.0 which is incompatible.\n",
      "tfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\n",
      "tensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.19.2\n",
      "Collecting tensorflow-data-validation\n",
      "  Downloading tensorflow_data_validation-1.9.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: apache-beam[gcp]<3,>=2.38 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (2.40.0)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5\n",
      "  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib<0.15,>=0.12\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.13 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (3.20.1)\n",
      "Requirement already satisfied: tensorflow-metadata<1.10,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (1.9.0)\n",
      "Requirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (1.15.0)\n",
      "Requirement already satisfied: pyarrow<6,>=1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (5.0.0)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (1.3.5)\n",
      "Requirement already satisfied: tfx-bsl<1.10,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (1.9.0)\n",
      "Requirement already satisfied: absl-py<2.0.0,>=0.9 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (0.15.0)\n",
      "Collecting pyfarmhash<0.4,>=0.2\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.9/99.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation) (1.19.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.4.2)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httplib2<0.21.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.20.4)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.10.0.2)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.7.0)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.28.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2022.1)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.7.5)\n",
      "Requirement already satisfied: grpcio<2,>=1.33.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.47.0)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.5.2)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.7)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.20.6)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.1.0)\n",
      "Requirement already satisfied: google-auth-httplib2<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.1.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.34.4)\n",
      "Requirement already satisfied: google-cloud-pubsub<3,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-pubsublite<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.4.2)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.5.31)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.2-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m403.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery-storage>=2.6.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.13.2)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (4.2.4)\n",
      "Requirement already satisfied: google-cloud-dlp<4,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.7.1)\n",
      "Collecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.3-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.2.2)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.2-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.1/435.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.2-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.7/267.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-datastore<2,>=1.8.0\n",
      "  Downloading google_cloud_datastore-1.15.5-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.2/134.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<2,>=0.28.1\n",
      "  Downloading google_cloud_core-1.7.3-py2.py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.2.0)\n",
      "Collecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.3-py2.py3-none-any.whl (255 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.6/255.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.35.0)\n",
      "Collecting numpy<2,>=1.16\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.1.2)\n",
      "Collecting keras<2.10.0,>=2.9.0rc0\n",
      "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.6.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (21.3)\n",
      "Collecting absl-py<2.0.0,>=0.9\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.7/123.7 kB\u001b[0m \u001b[31m587.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (3.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (3.3.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.12.1)\n",
      "Collecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: flatbuffers<2,>=1.12 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.12)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (59.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (0.2.0)\n",
      "Collecting protobuf<4,>=3.13\n",
      "  Downloading protobuf-3.19.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata<1.10,>=1.9.0->tensorflow-data-validation) (1.56.3)\n",
      "Collecting google-api-python-client<2,>=1.7.11\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15 in /opt/conda/lib/python3.7/site-packages (from tfx-bsl<1.10,>=1.9.0->tensorflow-data-validation) (2.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (0.37.1)\n",
      "Collecting uritemplate<4dev,>=3.0.0\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.10,>=1.9.0->tensorflow-data-validation) (2.8.1)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (4.1.3)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.17.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.2.7)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (4.8)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.3.3)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.12.4)\n",
      "Requirement already satisfied: grpcio-status>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.47.0)\n",
      "Requirement already satisfied: overrides<7.0.0,>=6.0.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (6.1.0)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-1.32.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.6/93.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py>=2.9.0->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.5.2)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.6.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.21.0,>=0.8->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2022.6.15)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (2.1.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (0.4.6)\n",
      "  Downloading google_api_core-1.31.6-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-1.31.5-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core[grpc,grpcgcp] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-core[grpc,grpcgcp]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.8.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: google-api-core 2.8.2 does not provide the extra 'grpcgcp'\u001b[0m\u001b[33m\n",
      "\u001b[0mINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_api_core-2.8.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.8.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.3-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.2-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.7.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core[grpc,grpcgcp] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_api_core-2.6.1-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading google_api_core-2.6.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.5.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.4.0-py2.py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_api_core-2.3.2-py2.py3-none-any.whl (109 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (1.3.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (4.11.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client>=1.4.12->google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.4.8)\n",
      "Requirement already satisfied: typing-utils>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from overrides<7.0.0,>=6.0.1->google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (0.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (1.15.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (3.8.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5->tensorflow-data-validation) (3.2.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]<3,>=2.38->tensorflow-data-validation) (2.21)\n",
      "Building wheels for collected packages: pyfarmhash, dill\n",
      "  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp37-cp37m-linux_x86_64.whl size=108622 sha256=88890c3cb30fa3b864d64aa3f944ff0dab1d9f6b16808fddcbaa0238dbfe6323\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/53/58/7a/3b040f3a2ee31908e3be916e32660db6db53621ce6eba838dc\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=5d3c0fc2b8a248f0b117b01c58968ab9e0b220a7e61d0616e2e79c754749df59\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "Successfully built pyfarmhash dill\n",
      "Installing collected packages: pyfarmhash, libclang, keras, joblib, uritemplate, tensorflow-io-gcs-filesystem, tensorflow-estimator, protobuf, numpy, dill, absl-py, google-api-core, tensorboard, google-cloud-core, google-api-python-client, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-language, google-cloud-datastore, google-cloud-bigtable, tensorflow-data-validation\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.6.0\n",
      "    Uninstalling keras-2.6.0:\n",
      "      Successfully uninstalled keras-2.6.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n",
      "      Successfully uninstalled joblib-1.0.1\n",
      "  Attempting uninstall: uritemplate\n",
      "    Found existing installation: uritemplate 4.1.1\n",
      "    Uninstalling uritemplate-4.1.1:\n",
      "      Successfully uninstalled uritemplate-4.1.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.6.0\n",
      "    Uninstalling tensorflow-estimator-2.6.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.6.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.5.1\n",
      "    Uninstalling dill-0.3.5.1:\n",
      "      Successfully uninstalled dill-0.3.5.1\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.15.0\n",
      "    Uninstalling absl-py-0.15.0:\n",
      "      Successfully uninstalled absl-py-0.15.0\n",
      "  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 2.8.1\n",
      "    Uninstalling google-api-core-2.8.1:\n",
      "      Successfully uninstalled google-api-core-2.8.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.6.0\n",
      "    Uninstalling tensorboard-2.6.0:\n",
      "      Successfully uninstalled tensorboard-2.6.0\n",
      "  Attempting uninstall: google-cloud-core\n",
      "    Found existing installation: google-cloud-core 2.3.1\n",
      "    Uninstalling google-cloud-core-2.3.1:\n",
      "      Successfully uninstalled google-cloud-core-2.3.1\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.52.0\n",
      "    Uninstalling google-api-python-client-2.52.0:\n",
      "      Successfully uninstalled google-api-python-client-2.52.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.6.4\n",
      "    Uninstalling tensorflow-2.6.4:\n",
      "      Successfully uninstalled tensorflow-2.6.4\n",
      "  Attempting uninstall: google-cloud-vision\n",
      "    Found existing installation: google-cloud-vision 2.7.3\n",
      "    Uninstalling google-cloud-vision-2.7.3:\n",
      "      Successfully uninstalled google-cloud-vision-2.7.3\n",
      "  Attempting uninstall: google-cloud-videointelligence\n",
      "    Found existing installation: google-cloud-videointelligence 2.7.1\n",
      "    Uninstalling google-cloud-videointelligence-2.7.1:\n",
      "      Successfully uninstalled google-cloud-videointelligence-2.7.1\n",
      "  Attempting uninstall: google-cloud-spanner\n",
      "    Found existing installation: google-cloud-spanner 3.15.1\n",
      "    Uninstalling google-cloud-spanner-3.15.1:\n",
      "      Successfully uninstalled google-cloud-spanner-3.15.1\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.4.3\n",
      "    Uninstalling google-cloud-language-2.4.3:\n",
      "      Successfully uninstalled google-cloud-language-2.4.3\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 2.7.1\n",
      "    Uninstalling google-cloud-datastore-2.7.1:\n",
      "      Successfully uninstalled google-cloud-datastore-2.7.1\n",
      "  Attempting uninstall: google-cloud-bigtable\n",
      "    Found existing installation: google-cloud-bigtable 2.10.1\n",
      "    Uninstalling google-cloud-bigtable-2.10.1:\n",
      "      Successfully uninstalled google-cloud-bigtable-2.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-io 0.21.0 requires tensorflow<2.7.0,>=2.6.0, but you have tensorflow 2.9.1 which is incompatible.\n",
      "tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, but you have tensorflow-io-gcs-filesystem 0.26.0 which is incompatible.\n",
      "pandas-profiling 3.1.0 requires joblib~=1.0.1, but you have joblib 0.14.1 which is incompatible.\n",
      "google-cloud-storage 2.4.0 requires google-cloud-core<3.0dev,>=2.3.0, but you have google-cloud-core 1.7.3 which is incompatible.\n",
      "cloud-tpu-client 0.10 requires google-api-python-client==1.8.0, but you have google-api-python-client 1.12.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-1.1.0 dill-0.3.1.1 google-api-core-1.32.0 google-api-python-client-1.12.11 google-cloud-bigtable-1.7.2 google-cloud-core-1.7.3 google-cloud-datastore-1.15.5 google-cloud-language-1.3.2 google-cloud-spanner-1.19.3 google-cloud-videointelligence-1.16.3 google-cloud-vision-1.0.2 joblib-0.14.1 keras-2.9.0 libclang-14.0.1 numpy-1.21.6 protobuf-3.19.4 pyfarmhash-0.3.2 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-data-validation-1.9.0 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 uritemplate-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow==5.0.0\n",
    "!pip install numpy==1.19.2\n",
    "!pip install tensorflow-data-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "**Restart the kernel (Kernel > Restart kernel > Restart).**\n",
    "\n",
    "**Re-run the above cell and proceed further.**\n",
    "\n",
    "**Note: Please ignore any incompatibility warnings and errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing TensorFlow Data Validation\n",
      "TFDV version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_data_validation as tfdv\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Installing TensorFlow Data Validation')\n",
    "!pip install -q tensorflow_data_validation[visualization]\n",
    "\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fnm6Mj3vTGLm"
   },
   "source": [
    "###  Load the Consumer Spending Dataset\n",
    "\n",
    "We will download our dataset from Google Cloud Storage. The columns in the dataset are:\n",
    "\n",
    "* 'Graduated': Whether or not the person is a college graduate\n",
    "* 'Work Experience': The number of years in the workforce\n",
    "* 'Family Size': The size of the family unit\n",
    "* 'Spending Score': The spending score for consumer spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No     Healthcare              1.0          4.0            Low\n",
       "1       Yes       Engineer              NaN          3.0        Average\n",
       "2       Yes       Engineer              1.0          1.0            Low\n",
       "3       Yes         Lawyer              0.0          2.0           High\n",
       "4       Yes  Entertainment              NaN          6.0           High"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a comma-separated values (csv) file into DataFrame.\n",
    "# TODO: Your code goes here\n",
    "score_train = pd.read_csv('data/score_train.csv')\n",
    "score_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Executive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Artist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No         Doctor              0.0          5.0        Average\n",
       "1       Yes  Entertainment              1.0          4.0        Average\n",
       "2        No         Lawyer              0.0          5.0            Low\n",
       "3       Yes      Executive              1.0          5.0           High\n",
       "4       Yes         Artist              1.0          2.0        Average"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a comma-separated values (csv) file into DataFrame.\n",
    "# TODO: Your code goes here\n",
    "score_test = pd.read_csv('data/score_test.csv')\n",
    "score_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Graduated        3964 non-null   object \n",
      " 1   Profession       3944 non-null   object \n",
      " 2   Work_Experience  3589 non-null   float64\n",
      " 3   Family_Size      3831 non-null   float64\n",
      " 4   Spending_Score   4000 non-null   object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "score_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the methods present in TFDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CombinerStatsGenerator',\n",
       " 'CrossFeatureView',\n",
       " 'DatasetListView',\n",
       " 'DatasetView',\n",
       " 'DetectFeatureSkew',\n",
       " 'FeaturePath',\n",
       " 'FeatureView',\n",
       " 'GenerateStatistics',\n",
       " 'MergeDatasetFeatureStatisticsList',\n",
       " 'StatsOptions',\n",
       " 'TransformStatsGenerator',\n",
       " 'WriteStatisticsToBinaryFile',\n",
       " 'WriteStatisticsToRecordsAndBinaryFile',\n",
       " 'WriteStatisticsToTFRecord',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'anomalies',\n",
       " 'api',\n",
       " 'arrow',\n",
       " 'coders',\n",
       " 'compare_slices',\n",
       " 'constants',\n",
       " 'default_sharded_output_suffix',\n",
       " 'default_sharded_output_supported',\n",
       " 'display_anomalies',\n",
       " 'display_schema',\n",
       " 'experimental_get_feature_value_slicer',\n",
       " 'generate_statistics_from_csv',\n",
       " 'generate_statistics_from_dataframe',\n",
       " 'generate_statistics_from_tfrecord',\n",
       " 'get_domain',\n",
       " 'get_feature',\n",
       " 'get_feature_stats',\n",
       " 'get_slice_stats',\n",
       " 'get_statistics_html',\n",
       " 'infer_schema',\n",
       " 'load_anomalies_text',\n",
       " 'load_schema_text',\n",
       " 'load_sharded_statistics',\n",
       " 'load_statistics',\n",
       " 'load_stats_binary',\n",
       " 'load_stats_text',\n",
       " 'pywrap',\n",
       " 'set_domain',\n",
       " 'skew',\n",
       " 'statistics',\n",
       " 'types',\n",
       " 'update_schema',\n",
       " 'utils',\n",
       " 'validate_examples_in_csv',\n",
       " 'validate_examples_in_tfrecord',\n",
       " 'validate_statistics',\n",
       " 'version',\n",
       " 'visualize_statistics',\n",
       " 'write_anomalies_text',\n",
       " 'write_schema_text',\n",
       " 'write_stats_text']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check methods present in tfdv\n",
    "# TODO: Your code goes here\n",
    "[methods for methods in dir(tfdv)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing data with TFDV\n",
    "The usual workflow when using TFDV during training is as follows:\n",
    "\n",
    "\n",
    "1.   Generate statistics for the data\n",
    "2.   Use those statistics to generate a schema for each feature\n",
    "3.   Visualize the schema and statistics and manually inspect them\n",
    "4.   Update the schema if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize statistics\n",
    "\n",
    "First we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\n",
    "\n",
    "TFDV can compute descriptive [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses [Apache Beam](https://beam.apache.org/)'s data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation.\n",
    "\n",
    "**NOTE:  Compute statistics**\n",
    "* [tfdv.generate_statistics_from_csv](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv)\n",
    "* [tfdv.generate_statistics_from_dataframe](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe)\n",
    "* [tfdv.generate_statistics_from_tfrecord](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Statistics from a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data statistics for the input pandas DataFrame.\n",
    "# TODO: Your code goes here\n",
    "stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses [Facets](https://pair-code.github.io/facets/) to create a succinct visualization of our training data:\n",
    "\n",
    "* Notice that numeric features and categorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n",
    "* Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.\n",
    "* Notice that there are no examples with values for `pickup_census_tract`.  This is an opportunity for dimensionality reduction!\n",
    "* Try clicking \"expand\" above the charts to change the display\n",
    "* Try hovering over bars in the charts to display bucket ranges and counts\n",
    "* Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n",
    "* Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the input statistics using Facets.\n",
    "# TODO: Your code goes here\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV generates different types of statistics based on the type of features.\n",
    "\n",
    "**For numerical features, TFDV computes for every feature:**\n",
    "* Count of records\n",
    "* Number of missing (i.e. null values)\n",
    "* Histogram of values\n",
    "* Mean and standard deviation\n",
    "* Minimum and maximum values\n",
    "* Percentage of zero values\n",
    "\n",
    "**For categorical features, TFDV provides:**\n",
    "* Count of values\n",
    "* Percentage of missing values\n",
    "* Number of unique values\n",
    "* Average string length\n",
    "* Count for each label and its rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the score_train and the score_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)\n",
    "test_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_test)\n",
    "\n",
    "tfdv.visualize_statistics(\n",
    "  lhs_statistics=train_stats, lhs_name='TRAIN_DATASET',\n",
    "  rhs_statistics=test_stats, rhs_name='NEW_DATASET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVR02-y4V0uM"
   },
   "source": [
    "### Infer a schema\n",
    "\n",
    "Now let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
    "\n",
    "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Schema\n",
    "Once statistics are generated, the next step is to generate a schema for our dataset. This schema will map each feature in the dataset to a type (float, bytes, etc.). Also define feature boundaries (min, max, distribution of values and missings, etc.).\n",
    "\n",
    "Link to infer schema\n",
    "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema\n",
    "\n",
    "With TFDV, we generate schema from statistics using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LLkRJThVr9m"
   },
   "outputs": [],
   "source": [
    "# Infers schema from the input statistics.\n",
    "# TODO: Your code goes here\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV provides a API to print a summary of each feature schema using\n",
    "\n",
    "In this visualization, the columns stand for:\n",
    "\n",
    "**Type** indicates the feature datatype.\n",
    "\n",
    "**Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).\n",
    "\n",
    "**Valency** indicates the number of values required per training example. \n",
    "\n",
    "**Domain and Values** indicates The feature domain and its values\n",
    "\n",
    "In the case of categorical features, single indicates that each training example must have exactly one category for the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Schema \n",
    "As stated above, **Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).  Currently, all of our features except for our target label are shown as \"optional\". We need to make our features all required except for \"Work Experience\".  We will need to update the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFDV lets you update the schema according to your domain knowledge of the data if you are not satisfied by the auto-generated schema.  We will update three use cases:  Making a feature required, adding a value to a feature, and change a feature from a float to an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change optional features to required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size from FLOAT to Int\n",
    "Graduated_feature = tfdv.get_feature(schema, 'Graduated')\n",
    "Graduated_feature.presence.min_fraction = 1.0\n",
    "Profession_feature = tfdv.get_feature(schema, 'Profession')\n",
    "Profession_feature.presence.min_fraction = 1.0\n",
    "Family_Size_feature = tfdv.get_feature(schema, 'Family_Size')\n",
    "Family_Size_feature.presence.min_fraction = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update a feature with a new value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add \"self-employed\" to the Profession feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.insert(0, 'Self-Employed')\n",
    "Profession_domain.value\n",
    "# [0 indicates I want 'Self-Employed to come first', if the number were 3, \n",
    "# it would be placed after the third value. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove \"Homemaker\" from \"Profession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.remove('Homemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change a feature from a float to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size to Int\n",
    "size = tfdv.get_feature(schema, 'Family_Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size.type=2\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, you compare two datasets and check for anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8eC59yISdGB"
   },
   "source": [
    "## When to use TFDV\n",
    "\n",
    "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here are a few more:\n",
    "\n",
    "* Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
    "* Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
    "* Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/transform)) to make sure we haven't done something wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/mlops-on-gcp/blob/master/examples/tfdv-structured-data/tfdv-covertype.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "tfdv_basic.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
